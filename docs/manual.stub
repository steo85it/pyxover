How to process altimetry data in PyXovers/Geoloc
------------------------------------------------

prOpt: set most processing options (local, parallel, debug, partials) and perturbation
to orbit and geophysics + select folder and terrain roughness
other options depend on the parameters set at launch (see below)

Processing
----------

1. prOpt: set options for crossovers analysis
	    local = 0/1 (defines out/tmp dir and options)
	    datasimopt = 'data'/'sim' to choose if simulating data (and getting them from
			sim data folder) or from data --> everything is sim now, could remove
	    parallel = run selected steps using multiprocessing.Pool routines
	    partials = compute partials w.r.t. parOrb and parGlo (also selected in the same file). If not,
	    only get residuals, no sol.
	    expopt = e.g., 'KX0' defines the name of the experiment and input (datasimopt) /output folders (via outdir
	    in same folder). Data folder should be filled and created manually (see below "Prepare files locally")
        Select partials to compute, constraints and parameters to estimate.
        cloop_sim = if closed loop simulation, apply perturbations as defined here
        SpInterp = use spice (no parallel) or interpolate spice data/use existing interpolation
        new_gtrack, new_xov = if 0, ignore, if 1 create if not already existing, if 2 create and overwrite
        Vecopt contains the main info about the central body (name, radius, etc...) for general purpose and ephemeris

Following steps can be run manually or via mla_iter, which runs geoloc, xovers, accum for a set of MMYY, combinations and iterations.
1. geolocalisation of data:
   set configuration to run python3 launch_test X YYMM 1, where X = 0 unless when testing
   the impact of different topographies; YYMM is the month to process; 1 launches the
   PyAltSim (if data_sim = sim and sim = 1 in prOpt) and PyGeoloc in series.
   if no data selected: - check if "bad data" (channel > 4)
			- check if spice window is selected
   if simulating data, partials (prOpt) should be set to 0 to save time

2. xovers computation and processing: keep launch_set settings, just relaunch by
   setting configuration to run python3 launch_test X Y 2, where X = 0 unless when testing
   the impact of different topographies; Y is the grid element (YYMM vs YYMM) to process; 
   2 launches the xover analysis
   Xovers analysis is based on gtrack files generated in prOpt.outdir by PyGeoloc (step 1).
   Depending on settings (prOpt.compute_input_xov and xov_prc_iters.import_abmat), the xovers location
   is either computed using all combinations of subsampled tracks (time consuming) or imported from previous knowledge.
   Then, subtracks are extracted from gtracks and stacked togheter, projected around preliminary xovers coordinates and
   processed to get refined location and partial derivatives w.r.t. chosen parameters (in prOpt). Output is a pkl
   containing all required info for least-square solution of orbit and geophysical parameters improvement.

Closed-loop simulation
----------------------
== SIM ==
1. prOpt: set sim_altdata = 1, partials = 0, parallel = 0, topo and range_noise as desired
   launch_test: see above, set data_sim = 'sim'
--> creates simulated altimetry data file (based on illumng results, imported or locally computed - see prOpt)
1a. if illumng data do not exist (a priori geolocalisation of the tracks), to create them:
   select new_illumNG = 1 to create tmp/epo_mla_YYMM.in
   cd /home/sberton2/projects/Mercury_tides/_MLA_Stefano; rm tmp; modify doSLURM to process the desired dates and launch
   creates /explore/nobackup/people/sberton2/MLA/aux/illumng/mlatimes_YYMM/boresightxxx, which is used as input for sim run
2. /home/sberton2/launchLISTslurm loadPyAltSim PyAltSim 1 23:59:59 30Gb 20: launch process for selected months (after preparing list of commands in PyAltSim
3. Preparing new apriori (illumNG or internal spherical) is time consuming: if already available in tmpdir, just switch new_illumNG=False and proceed to simulating data.
## pyproj warning/error: export PROJ_LIB=$NOBACKUP/.spack_repo/opt/spack/linux-debian8-broadwell/gcc-9.3.0/proj-6.3.1-wosgaaf7fqh5u7iaiu4ucpbkyk2l5tv4/share/proj or same path with proj.set_path(...) (or similar)

== PROC ==
3. set the perturbation you want to apply to a-priori orbits or geophysical parameters

4. launch mla_iter, automatically looping over 3 steps:
	3a. PyGeoloc: reads the (simulated) data, perturbs what needs to be and geolocalises data
	    on the surface. If iter > 0, then the solution from the previous iteration is 
	    summed to the data.
	3b. PyXover: uses geolocalised data stored in orbit-wise pkl files generated by PyGeoloc
	    to generate xovers and partials.
	3c. AccumXov: combining the partial matrix in sparse form and the respective residuals.
            Solving for a given set of parameters (all or selected )

Launch scripts on pgda
----------------------
From the program directory, run:
/home/sberton2/launchLISTslurm loadPyAltSim PyAltSim

Evaluate/plot results
---------------------


Prepare files locally (data selection)
--------------------------------------

Copy full dataset and orbits to data and aux, respectively.
Launch selection script (currently scratch_12) with the data dir.
Copy data to experiment folder by
for f in /home/sberton2/Works/NASA/Mercury_tides/data/SIM_??/; do mkdir $f/KX1; cp -rs $f/KX2/* $f/KX1/.; done
or
for f in /explore/nobackup/people/sberton2/MLA/data/SIM_??/; do mkdir $f/KX2; cp -rs $f/KX1/* $f/KX2/.; done
Update launch_test

Import new orbit to pkl
-----------------------
Required to parallelise PyGeoloc
PrOpt: turn off parallel and partials, set spInterp=2, spauxdir='desired_spk_dir_in_auxdir' (empty)
mla_iter: set 1 iteration PyGeoloc only
PyGeoloc: set furnsh to desired file

Produce topo from altimetry residuals
-------------------------------------
Automatised in lib/gtrack2dem.py : select input exp and run
Backup solution only: ... in lib/ (sends to slurm, which is somehow good)
plt_altres.py : change name of input and output to generate concat_ladata
plot_res.sh: launches gmt reading and processing to grd (change input/output in ladata2grd.sh)
go back to gtrack2dem for last step

Profiling
---------
Decorate routines with #@profile; then launch with, e.g.,
kernprof -v -l launch_test.py 0 0 3 0   # for time profiling (spack load py-line-profiler)
python3 -m memory_profiler launch_test.py 0 0 3 0   # for memory profiling
mprof run --include-children --multiprocess launch_test.py 0 20 2 1 # time-wise memory profiling
N.B.: does not work when launching from within pycharm (or would need config - maybe just include memory_profile)
Then plot using "snakeviz program.prof" (https://jiffyclub.github.io/snakeviz/)
